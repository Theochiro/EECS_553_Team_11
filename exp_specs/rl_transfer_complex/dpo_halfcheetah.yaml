meta_data:
  script_path: run_scripts/adv_irl_lfo_pretrain_exp_script.py
  exp_name: dpo_halfcheetah_rl_bigbuffer_pretrain_complex
  description: Train an dpo model
  num_workers: 5 # 64
  num_gpu_per_worker: 1
# -----------------------------------------------------------------------------
variables:
  adv_irl_params:
    grad_pen_weight: [4.0]
    state_predictor_alpha: [0.0]
    num_inverse_dynamic_updates_per_loop_iter: [0]
    num_train_calls_between_inverse_dynamic_training: [10]
    inv_noise: [false]
    inv_buffer: [false]
    # inv_buf_size: [100000]
    deterministic_sp: [false]
    deterministic_inv: [false]
    epsilon: [0.99]
    epsilon_ratio: [0.1]
    valid_ratio: [0.2]
    replay_buffer_size: [1000000]
    # max_num_inverse_dynamic_updates_per_loop_iter: [50]
    rew_shaping: [true]
    inverse_dynamic_lr: [0.0001]
    num_pretrain_updates: [0]
    pretrain_steps_per_epoch: [5000]
    pretrain_inv_num: [100]
  sac_params:
    reward_scale: [2.0]
    # train_alpha: [true]
    alpha: [0.0]
    qscale: [true]
    norm_q: [true]
    q_norm_mode: ["min_max"]
    clip_q: [false]
    sp_pg_weight: [0.1]
  seed: [0,1,2,3,4]

# -----------------------------------------------------------------------------
constants:
  changing_dynamics: true
  expert_name: 'halfcheetah_sac'
  expert_idx: 0
  scale_env_with_demo_stats: true
  traj_num: 4
  policy_checkpoint: "logs/dpo-halfcheetah-rl-bigbuffer--state-diff-unionsp--gp-4.0--spalpha-0.0--idlr-0.0001--rs-2.0--inviter-0--invevery-10/dpo_halfcheetah_rl_bigbuffer--state_diff-unionsp--gp-4.0--spalpha-0.0--idlr-0.0001--rs-2.0--inviter-0--invevery-10_2022_01_23_08_29_59_0000--s-0/params.pkl"

  disc_num_blocks: 2
  disc_hid_dim: 128
  disc_hid_act: tanh
  disc_use_bn: false
  disc_clamp_magnitude: 10.0

  policy_net_size: 256
  policy_num_hidden_layers: 2

  invdy_net_size: 512
  invdy_num_hidden_layers: 4

  sp_net_size: 256
  sp_num_hidden_layers: 2

  adv_irl_params:
    mode: 'rl'
    inverse_mode: 'MSE'
    state_predictor_mode: 'MLE'
    state_only: true
    state_diff: true
    union: false
    union_sp: true
    sas: false
    qss: false
    # inv_buffer: false
    # inv_buffer: true
    # deterministic_sp: false
    # deterministic_inv: false
    # epsilon: 0.0

    num_epochs: 202
    num_steps_per_epoch: 10000
    num_steps_between_train_calls: 1000
    max_path_length: 1000
    min_steps_before_training: 5000

    eval_deterministic: true
    num_steps_per_eval: 10000

    # replay_buffer_size: 10000
    no_terminal: false
    eval_no_terminal: false
    wrap_absorbing: false

    num_update_loops_per_train_call: 1000
    num_disc_updates_per_loop_iter: 0
    num_policy_updates_per_loop_iter: 0
    num_state_predictor_updates_per_loop_iter: 0
    # num_inverse_dynamic_updates_per_loop_iter: 10
    # num_pretrain_updates: 0
    # pretrain_steps_per_epoch: 5000

    disc_lr: 0.0003
    disc_momentum: 0.9
    use_grad_pen: true
    use_wgan: false
    # grad_pen_weight: 10.0
    disc_optim_batch_size: 256
    policy_optim_batch_size: 256
    policy_optim_batch_size_from_expert: 0

    state_predictor_lr: 0.0003
    state_predictor_momentum: 0.9
    inverse_dynamic_lr: 0.001
    inverse_dynamic_momentum: 0.9

    save_best: true
    freq_saving: 20
    save_replay_buffer: false
    save_environment: false
    save_algorithm: false

  sac_params:
    # reward_scale: 8.0
    discount: 0.99
    soft_target_tau: 0.005
    beta_1: 0.25
    policy_lr: 0.0003
    qf_lr: 0.0003
    policy_mean_reg_weight: 0.001
    policy_std_reg_weight: 0.001

  env_specs:
    env_name: 'halfcheetah'
    env_kwargs: {}
